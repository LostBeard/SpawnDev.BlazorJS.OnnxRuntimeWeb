@page "/gpu-preprocessing"
@using global::ILGPU
@using global::ILGPU.Runtime
@using SpawnDev.ILGPU
@using SpawnDev.ILGPU.WebGPU
@using SpawnDev.ILGPU.WebGPU.Backend
@using System.Text.Json
@implements IAsyncDisposable

<PageTitle>GPU Preprocessing ‚Äî ILGPU + ORT</PageTitle>

<h1 style="margin-bottom: 0.25rem;">üöÄ GPU Image Preprocessing</h1>
<p class="text-secondary" style="margin-bottom: 1.5rem;">
    Compare <strong>CPU</strong> vs <strong>GPU (ILGPU WebGPU)</strong> image preprocessing for ONNX inference.
    Normalize and transpose images at various resolutions to see GPU parallelism in action.
</p>

<div style="display: flex; gap: 2rem; flex-wrap: wrap; align-items: flex-start;">

    <!-- Controls -->
    <div style="min-width: 280px;">

        <div style="position: relative; width: 260px; height: 260px; background: var(--bg-card); border: 1px solid var(--border-color);
                    border-radius: var(--radius-lg); display: flex; align-items: center; justify-content: center; overflow: hidden; margin-bottom: 0.75rem;">
            @if (_imageDataUrl != null)
            {
                <img src="@_imageDataUrl" style="max-width: 100%; max-height: 100%; object-fit: contain;" />
            }
            else
            {
                <span class="text-muted" style="font-size: 0.9rem;">No image selected</span>
            }
        </div>

        <div style="display: flex; gap: 0.5rem; flex-wrap: wrap; margin-bottom: 0.75rem;">
            <label class="btn btn-primary" style="cursor: pointer; margin: 0;">
                Upload Image
                <InputFile OnChange="OnFileSelected" accept="image/*" style="display: none;" />
            </label>
            <button class="btn-outline" @onclick="RunBenchmark" disabled="@(_loading || _imageDataUrl == null || !_ready)">
                @(_loading ? "Running..." : "Run Benchmark")
            </button>
        </div>

        <!-- Resolution selector -->
        <div style="margin-bottom: 0.75rem;">
            <label class="text-muted" style="font-size: 0.8rem; margin-right: 0.5rem;">Resolution:</label>
            <select @bind="_selectedResolution" style="background: var(--bg-card); color: var(--text-primary); border: 1px solid var(--border-color);
                        border-radius: var(--radius-sm); padding: 0.25rem 0.5rem; font-size: 0.85rem;">
                @foreach (var res in _resolutions)
                {
                    <option value="@res">@(res)√ó@(res) (@(((long)res * res * 4 / 1024).ToString("N0")) KB)</option>
                }
            </select>
        </div>

        @if (_statusMessage != null)
        {
            <p class="text-secondary" style="font-size: 0.85rem;">@_statusMessage</p>
        }

        <!-- Sample Images -->
        <div style="margin-top: 1rem;">
            <p class="text-muted" style="font-size: 0.8rem; margin-bottom: 0.5rem;">Or try a sample:</p>
            <div style="display: flex; gap: 0.5rem; flex-wrap: wrap;">
                @foreach (var sample in _sampleImages)
                {
                    <button class="btn-outline" style="font-size: 0.8rem; padding: 0.3rem 0.6rem;"
                            @onclick="() => LoadSample(sample)" disabled="@(!_ready || _loading)">
                        @sample.Name
                    </button>
                }
            </div>
        </div>
    </div>

    <!-- Results -->
    <div style="min-width: 350px; flex: 1;">

        @if (_results != null)
        {
            <h3 style="margin-bottom: 0.75rem;">Benchmark Results <span class="text-muted" style="font-size: 0.85rem; font-weight: 400;">(@(_results.Resolution)√ó@(_results.Resolution) ‚Äî @(_results.PixelCount.ToString("N0")) pixels)</span></h3>

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin-bottom: 1.5rem;">
                <div class="card">
                    <div class="card-header" style="font-size: 0.9rem;">üñ•Ô∏è CPU Preprocessing</div>
                    <div style="font-size: 2rem; font-weight: 700; color: var(--text-primary);">
                        @($"{_results.CpuPreprocessMs:F1}") <span style="font-size: 0.9rem; color: var(--text-muted);">ms</span>
                    </div>
                    <div class="text-muted" style="font-size: 0.8rem; margin-top: 0.25rem;">
                        Inference: @($"{_results.CpuInferenceMs:F1}") ms
                    </div>
                </div>
                <div class="card" style="border-color: rgba(88,166,255,0.3);">
                    <div class="card-header" style="font-size: 0.9rem;">‚ö° GPU Preprocessing</div>
                    <div style="font-size: 2rem; font-weight: 700; color: var(--accent);">
                        @($"{_results.GpuPreprocessMs:F1}") <span style="font-size: 0.9rem; color: var(--text-muted);">ms</span>
                    </div>
                    <div class="text-muted" style="font-size: 0.8rem; margin-top: 0.25rem;">
                        Inference: @($"{_results.GpuInferenceMs:F1}") ms
                    </div>
                </div>
            </div>

            @if (_results.Speedup > 0)
            {
                <div class="card" style="margin-bottom: 1rem;">
                    <div style="display: flex; justify-content: space-between; align-items: center;">
                        <span class="text-secondary">Preprocessing Speedup</span>
                        <span style="font-size: 1.25rem; font-weight: 700; color: @(_results.Speedup >= 1 ? "var(--success)" : "var(--warning)");">
                            @($"{_results.Speedup:F1}")√ó
                        </span>
                    </div>
                </div>
            }

            <h4 style="margin-bottom: 0.5rem;">Result Verification</h4>
            <p style="font-size: 0.85rem; color: @(_results.ResultsMatch ? "var(--success)" : "var(--danger)");">
                @(_results.ResultsMatch ? "‚úÖ CPU and GPU inference results match!" : "‚ùå Results differ ‚Äî check implementation")
            </p>

            @if (_results.CpuTopClass != null)
            {
                <div style="margin-top: 0.5rem; font-size: 0.85rem;">
                    <p><span class="text-muted">CPU Top Class:</span> <strong>@_results.CpuTopClass</strong> (@($"{_results.CpuTopConfidence:F1}")%)</p>
                    <p><span class="text-muted">GPU Top Class:</span> <strong>@_results.GpuTopClass</strong> (@($"{_results.GpuTopConfidence:F1}")%)</p>
                </div>
            }
        }
        else if (!_ready)
        {
            <div class="card" style="text-align: center; padding: 2rem;">
                <p class="text-secondary">@(_statusMessage ?? "Initializing...")</p>
            </div>
        }
        else
        {
            <div class="card" style="text-align: center; padding: 2rem;">
                <p class="text-secondary">Upload an image and click Run Benchmark</p>
            </div>
        }
    </div>

</div>


@code {
    [Inject] BlazorJSRuntime JS { get; set; } = default!;

    record BenchmarkResult(
        double CpuPreprocessMs, double CpuInferenceMs,
        double GpuPreprocessMs, double GpuInferenceMs,
        double Speedup, bool ResultsMatch,
        string? CpuTopClass, float CpuTopConfidence,
        string? GpuTopClass, float GpuTopConfidence,
        int Resolution, int PixelCount);

    record SampleImage(string Name, string Url);

    private OnnxRuntime? _ort;
    private OrtInferenceSession? _cpuSession;   // WASM EP ‚Äî for CPU tensor inference
    private OrtInferenceSession? _gpuSession;   // WebGPU EP ‚Äî for GPU-resident inference
    private Context? _ilgpuContext;
    private Accelerator? _accelerator;
    private string? _imageDataUrl;
    private string? _statusMessage;
    private bool _loading;
    private bool _ready;
    private BenchmarkResult? _results;
    private int _selectedResolution = 1024;
    private Action<Index1D, ArrayView<float>, ArrayView<float>, int, int>? _cachedKernel;

    private static readonly int[] _resolutions = { 224, 512, 1024, 2048 };

    // Local sample images ‚Äî no external dependencies
    private readonly SampleImage[] _sampleImages = new[]
    {
        new SampleImage("üê± Cat", "images/cat.jpg"),
        new SampleImage("üêï Dog", "images/dog.jpg"),
        new SampleImage("üöó Car", "images/car.jpg"),
    };

    // Full ImageNet 1000-class labels loaded from JSON
    private static string[]? _imagenetLabels;

    protected override async Task OnAfterRenderAsync(bool firstRender)
    {
        if (!firstRender) return;

        try
        {
            // Step 1: Init ORT and create WebGPU session FIRST
            // ORT's WebGPU EP creates its own GPUDevice during session init.
            // We'll extract that device and share it with ILGPU.
            _statusMessage = "Loading ONNX Runtime...";
            StateHasChanged();
            _ort = await OnnxRuntime.Init();

            // Load ImageNet labels if not already cached
            if (_imagenetLabels == null)
            {
                _statusMessage = "Loading ImageNet labels...";
                StateHasChanged();
                using var labelsResponse = await JS.Fetch("models/imagenet_labels.json");
                var labelsJson = await labelsResponse.Text();
                _imagenetLabels = JsonSerializer.Deserialize<string[]>(labelsJson) ?? new string[0];
            }

            // Load model
            _statusMessage = "Loading SqueezeNet model...";
            StateHasChanged();
            using var response = await JS.Fetch("models/squeezenet/squeezenet1.0-12.onnx");
            using var arrayBuffer = await response.ArrayBuffer();

            // Create WASM session for CPU inference (no WebGPU needed)
            _cpuSession = await _ort.CreateInferenceSessionAsync(arrayBuffer, new SessionCreateOptions
            {
                ExecutionProviders = new[] { "wasm" },
                LogSeverityLevel = 3
            });

            // Create WebGPU session ‚Äî this triggers ORT to create its own GPUDevice
            _statusMessage = "Creating WebGPU inference session...";
            StateHasChanged();
            _gpuSession = await _ort.CreateInferenceSessionAsync(arrayBuffer, new SessionCreateOptions
            {
                ExecutionProviders = new[] { "webgpu" },
                LogSeverityLevel = 3
            });

            // Step 2: Extract ORT's GPUDevice (populated after WebGPU session creation)
            _statusMessage = "Initializing ILGPU with ORT's WebGPU device...";
            StateHasChanged();

            using var env = _ort.Env;
            var ortDevice = env.GetWebGpuDevice();
            if (ortDevice == null)
                throw new InvalidOperationException("ORT did not create a WebGPU device. Cannot share device with ILGPU.");

            // Step 3: Create ILGPU context and accelerator using ORT's GPUDevice
            // This ensures both ILGPU and ORT use the exact same device instance,
            // enabling zero-copy buffer sharing via Tensor.fromGpuBuffer.
            _ilgpuContext = await Context.CreateAsync(b => b.WebGPU());
            _accelerator = WebGPUAccelerator.CreateFromExternalDevice(_ilgpuContext, ortDevice);

            // Cache the kernel so it's not recompiled on every benchmark run
            _cachedKernel = _accelerator.LoadAutoGroupedStreamKernel<Index1D, ArrayView<float>, ArrayView<float>, int, int>(NormalizeTransposeKernel);

            // Warm-up: run a tiny GPU dispatch to fully initialize the pipeline
            _statusMessage = "Warming up GPU pipeline...";
            StateHasChanged();
            using (var warmInput = _accelerator.Allocate1D<float>(16))
            using (var warmOutput = _accelerator.Allocate1D<float>(12))
            {
                _cachedKernel((Index1D)4, warmInput.View, warmOutput.View, 2, 2);
                await _accelerator.SynchronizeAsync();
            }

            _statusMessage = $"Ready ‚Äî ILGPU: {_accelerator.AcceleratorType}, {_accelerator.Name}";
            _ready = true;
        }
        catch (Exception ex)
        {
            _statusMessage = $"Init error: {ex.Message}";
            Console.WriteLine($"GPU Init Error: {ex}");
        }

        StateHasChanged();
    }

    async Task OnFileSelected(InputFileChangeEventArgs e)
    {
        var file = e.File;
        if (file == null) return;

        var buffer = new byte[file.Size];
        await file.OpenReadStream(maxAllowedSize: 10 * 1024 * 1024).ReadExactlyAsync(buffer);
        _imageDataUrl = $"data:{file.ContentType};base64,{Convert.ToBase64String(buffer)}";
        _results = null;
        StateHasChanged();

        // Auto-run benchmark
        await RunBenchmark();
    }

    async Task LoadSample(SampleImage sample)
    {
        _loading = true;
        _statusMessage = $"Loading {sample.Name}...";
        _results = null;
        StateHasChanged();
        await Task.Delay(10);

        try
        {
            using var response = await JS.Fetch(sample.Url);
            using var blob = await response.Blob();
            _imageDataUrl = URL.CreateObjectURL(blob);
            StateHasChanged();
            await RunBenchmark();
        }
        catch (Exception ex)
        {
            _statusMessage = $"Error loading sample: {ex.Message}";
            _loading = false;
            StateHasChanged();
        }
    }

    // ‚îÄ‚îÄ‚îÄ ILGPU Kernel: Normalize + NHWC‚ÜíNCHW transpose ‚îÄ‚îÄ‚îÄ
    // Uses float input (not byte) because WebGPU WGSL doesn't support array<u8> in storage buffers.
    // RGBA values are pre-converted to 0-255 floats on CPU before upload.
    static void NormalizeTransposeKernel(
        Index1D index,
        ArrayView<float> rgbaFloats,
        ArrayView<float> output,
        int width,
        int height)
    {
        int totalPixels = width * height;
        if (index >= totalPixels) return;

        int pixelIdx = index * 4;
        float r = rgbaFloats[pixelIdx] / 255f;
        float g = rgbaFloats[pixelIdx + 1] / 255f;
        float b = rgbaFloats[pixelIdx + 2] / 255f;

        // Normalize with ImageNet mean/std and write in NCHW order
        output[0 * totalPixels + index] = (r - 0.485f) / 0.229f; // R channel
        output[1 * totalPixels + index] = (g - 0.456f) / 0.224f; // G channel
        output[2 * totalPixels + index] = (b - 0.406f) / 0.225f; // B channel
    }

    async Task RunBenchmark()
    {
        if (_cpuSession == null || _gpuSession == null || _accelerator == null || _imageDataUrl == null) return;
        _loading = true;
        StateHasChanged();
        await Task.Delay(10);

        var res = _selectedResolution;

        try
        {
            // Get pixel data at the selected resolution
            _statusMessage = $"Rendering image at {res}√ó{res}...";
            StateHasChanged();

            using var canvas = new OffscreenCanvas(res, res);
            using var ctx = canvas.Get2DContext();
            using var img = await HTMLImageElement.CreateFromImageAsync(_imageDataUrl);
            ctx.DrawImage(img, 0, 0, res, res);
            using var imageData = ctx.GetImageData(0, 0, res, res);
            using var rgbaData = imageData.Data;
            var rgbaBytes = rgbaData.ReadBytes();

            int totalPixels = res * res;

            // Convert bytes to floats once ‚Äî shared by both CPU and GPU paths
            var rgbaFloats = new float[rgbaBytes.Length];
            for (int i = 0; i < rgbaBytes.Length; i++)
                rgbaFloats[i] = rgbaBytes[i];

            // ‚îÄ‚îÄ‚îÄ CPU Preprocessing ‚îÄ‚îÄ‚îÄ
            _statusMessage = $"CPU preprocessing {totalPixels:N0} pixels...";
            StateHasChanged();
            await Task.Delay(1);

            var swCpu = System.Diagnostics.Stopwatch.StartNew();
            var cpuOutput = CpuPreprocess(rgbaFloats, res, res);
            swCpu.Stop();
            var cpuPreprocessMs = swCpu.Elapsed.TotalMilliseconds;

            // ‚îÄ‚îÄ‚îÄ GPU Preprocessing (stays on GPU) ‚îÄ‚îÄ‚îÄ
            _statusMessage = $"GPU preprocessing {totalPixels:N0} pixels...";
            StateHasChanged();
            await Task.Delay(1);

            var swGpu = System.Diagnostics.Stopwatch.StartNew();
            using var gpuOutputBuffer = await GpuPreprocess(rgbaFloats, res, res);
            swGpu.Stop();
            var gpuPreprocessMs = swGpu.Elapsed.TotalMilliseconds;

            // ‚îÄ‚îÄ‚îÄ Inference uses 224√ó224 (model requirement) ‚îÄ‚îÄ‚îÄ
            // Re-render at 224√ó224 for actual model input
            float[] cpuInferenceInput;
            MemoryBuffer1D<float, Stride1D.Dense>? gpuInferenceBuffer = null;

            if (res == 224)
            {
                cpuInferenceInput = cpuOutput;
                // For GPU, reuse the already-computed buffer
                gpuInferenceBuffer = gpuOutputBuffer;
            }
            else
            {
                // Re-preprocess at 224√ó224 for inference (model requires it)
                using var canvas224 = new OffscreenCanvas(224, 224);
                using var ctx224 = canvas224.Get2DContext();
                ctx224.DrawImage(img, 0, 0, 224, 224);
                using var imgData224 = ctx224.GetImageData(0, 0, 224, 224);
                using var rgba224 = imgData224.Data;
                var bytes224 = rgba224.ReadBytes();
                var floats224 = new float[bytes224.Length];
                for (int i = 0; i < bytes224.Length; i++) floats224[i] = bytes224[i];
                cpuInferenceInput = CpuPreprocess(floats224, 224, 224);
                gpuInferenceBuffer = await GpuPreprocess(floats224, 224, 224);
            }

            // ‚îÄ‚îÄ‚îÄ CPU Inference ‚îÄ‚îÄ‚îÄ
            var swCpuInfer = System.Diagnostics.Stopwatch.StartNew();
            var cpuPredictions = await RunInference(cpuInferenceInput);
            swCpuInfer.Stop();
            var cpuInferenceMs = swCpuInfer.Elapsed.TotalMilliseconds;

            // ‚îÄ‚îÄ‚îÄ GPU-resident Inference (zero-copy from ILGPU buffer) ‚îÄ‚îÄ‚îÄ
            var swGpuInfer = System.Diagnostics.Stopwatch.StartNew();
            var gpuPredictions = await RunGpuInference(gpuInferenceBuffer!);
            swGpuInfer.Stop();
            var gpuInferenceMs = swGpuInfer.Elapsed.TotalMilliseconds;

            // Clean up if we allocated a separate inference buffer
            if (res != 224) gpuInferenceBuffer?.Dispose();

            // Verify results match
            bool match = cpuPredictions.topIdx == gpuPredictions.topIdx;
            double speedup = gpuPreprocessMs > 0 ? cpuPreprocessMs / gpuPreprocessMs : 0;

            _results = new BenchmarkResult(
                cpuPreprocessMs, cpuInferenceMs,
                gpuPreprocessMs, gpuInferenceMs,
                speedup, match,
                GetClassName(cpuPredictions.topIdx), cpuPredictions.topConf,
                GetClassName(gpuPredictions.topIdx), gpuPredictions.topConf,
                res, totalPixels);
        }
        catch (Exception ex)
        {
            _statusMessage = $"Error: {ex.Message}";
            Console.WriteLine($"Benchmark Error: {ex}");
        }
        finally
        {
            _loading = false;
            StateHasChanged();
        }
    }

    float[] CpuPreprocess(float[] rgbaFloats, int width, int height)
    {
        var output = new float[3 * width * height];
        int totalPixels = width * height;

        for (int i = 0; i < totalPixels; i++)
        {
            int pi = i * 4;
            output[0 * totalPixels + i] = (rgbaFloats[pi] / 255f - 0.485f) / 0.229f;
            output[1 * totalPixels + i] = (rgbaFloats[pi + 1] / 255f - 0.456f) / 0.224f;
            output[2 * totalPixels + i] = (rgbaFloats[pi + 2] / 255f - 0.406f) / 0.225f;
        }
        return output;
    }

    async Task<MemoryBuffer1D<float, Stride1D.Dense>> GpuPreprocess(float[] rgbaFloats, int width, int height)
    {
        int totalPixels = width * height;

        // Upload float RGBA to GPU
        using var gpuInput = _accelerator!.Allocate1D(rgbaFloats);

        // Output buffer (NCHW) ‚Äî NOT disposed here, caller owns it
        var gpuOutput = _accelerator.Allocate1D<float>(3 * totalPixels);

        // Run cached kernel (no recompilation)
        _cachedKernel!((Index1D)totalPixels, gpuInput.View, gpuOutput.View, width, height);
        await _accelerator.SynchronizeAsync();

        // Return the GPU buffer directly ‚Äî no CPU readback!
        return gpuOutput;
    }

    async Task<(int topIdx, float topConf)> RunInference(float[] preprocessed)
    {
        using var floatData = new Float32Array(preprocessed);
        using var tensor = new OrtTensor("float32", floatData, new long[] { 1, 3, 224, 224 });
        using var feeds = new OrtFeeds();
        feeds.Set(_cpuSession!.InputNames[0], tensor);
        using var result = await _cpuSession.Run(feeds);
        using var output = result.GetTensor(_cpuSession.OutputNames[0]);
        using var outputData = output.GetData<Float32Array>();
        return ExtractTopPrediction(outputData.ToArray());
    }

    /// <summary>
    /// GPU-resident inference: creates an ORT tensor directly from the ILGPU GPU buffer.
    /// No CPU readback ‚Äî data stays on the GPU the entire time.
    /// </summary>
    async Task<(int topIdx, float topConf)> RunGpuInference(MemoryBuffer1D<float, Stride1D.Dense> gpuBuffer)
    {
        // Extract the native WebGPU GPUBuffer from the ILGPU buffer
        var iView = (IArrayView)gpuBuffer;
        var webGpuBuffer = (WebGPUMemoryBuffer)iView.Buffer;
        var nativeGpuBuffer = webGpuBuffer.NativeBuffer.NativeBuffer!;

        // Create ORT tensor directly from the GPU buffer (zero-copy)
        using var tensor = _ort!.TensorFromGpuBuffer(nativeGpuBuffer, new TensorFromGpuBufferOptions
        {
            DataType = "float32",
            Dims = new long[] { 1, 3, 224, 224 }
        });

        using var feeds = new OrtFeeds();
        feeds.Set(_gpuSession!.InputNames[0], tensor);
        using var result = await _gpuSession.Run(feeds);
        using var output = result.GetTensor(_gpuSession.OutputNames[0]);

        // Output may be on GPU ‚Äî use GetDataAsync to download if needed
        using var outputData = await output.GetDataAsync<Float32Array>();
        return ExtractTopPrediction(outputData.ToArray());
    }

    static (int topIdx, float topConf) ExtractTopPrediction(float[] probs)
    {
        // SqueezeNet 1.0 output "softmaxout_1" is already softmaxed ‚Äî
        // values are probabilities (0..1), NOT raw logits.
        int topIdx = 0;
        float topVal = 0;
        for (int i = 0; i < probs.Length; i++)
        {
            float p = probs[i] * 100f;
            if (p > topVal) { topVal = p; topIdx = i; }
        }
        return (topIdx, topVal);
    }

    static string GetClassName(int idx)
    {
        if (_imagenetLabels != null && idx >= 0 && idx < _imagenetLabels.Length)
            return _imagenetLabels[idx];
        return $"class_{idx}";
    }

    public async ValueTask DisposeAsync()
    {
        _cpuSession?.Dispose();
        _gpuSession?.Dispose();
        _ort?.Dispose();
        _accelerator?.Dispose();
        _ilgpuContext?.Dispose();
    }
}
